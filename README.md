# Crypto Data Pipeline with Hadoop, Airflow, and Docker

## ğŸ“Œ Objectifs du Projet

Ce projet vise Ã  construire un pipeline complet de traitement des donnÃ©es de cryptomonnaies en utilisant des outils Big Data. Lâ€™objectif est de rÃ©cupÃ©rer, transformer, stocker et analyser les donnÃ©es de prix et de volume en temps quasi-rÃ©el.

### ğŸ› ï¸ Ã‰tapes principales :

### 1ï¸âƒ£ Ingestion des donnÃ©es :
- RÃ©cupÃ©ration quotidienne des prix et volumes des cryptomonnaies via lâ€™API CoinGecko.

### 2ï¸âƒ£ Stockage dans un Data Lake :
- Stockage des donnÃ©es brutes dans HDFS (Hadoop Distributed File System).

### 3ï¸âƒ£ Transformation et agrÃ©gation (MapReduce en Python) :
- Nettoyage des donnÃ©es (validation des champs, gestion des valeurs manquantes).
- Calcul de mÃ©triques : prix moyen, minimum, maximum, volume moyen, etc.
- GÃ©nÃ©ration dâ€™une sortie structurÃ©e au format CSV ou Parquet.

### 4ï¸âƒ£ Chargement dans HBase :
- Insertion des donnÃ©es traitÃ©es pour des requÃªtes rapides (ex : recherche par ID de crypto et date).

### 5ï¸âƒ£ Orchestration avec Apache Airflow :
- Gestion de la planification et de l'exÃ©cution automatisÃ©e du pipeline.

### ğŸ“š API Source : CoinGecko API

---

## ğŸ—ï¸ Architecture & Conception du Data Lake Hadoop

           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚       CoinGecko API    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚     DAG dâ€™Ingestion (Airflow)          â”‚
     â”‚ (Appel de lâ€™API + stockage dans HDFS)  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
              Zone Brute (Raw) dans HDFS
                        â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   DAG de Traitement (Airflow + MR)     â”‚
     â”‚ (MapReduce en Python : transformation  â”‚
     â”‚  et agrÃ©gation)                        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            Zone TraitÃ©/StructurÃ© dans HDFS
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               HBase              â”‚
        â”‚ (pour requÃªtes rapides/analyses) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                    Analytics / BI

---

## ğŸ³ DÃ©ploiement avec Docker & Docker Compose

Le projet est entiÃ¨rement conteneurisÃ© avec Docker pour simplifier le dÃ©ploiement. Les services sont orchestrÃ©s avec Docker Compose pour faciliter la mise en place de lâ€™environnement.

### ğŸ“‚ Structure des conteneurs :
- **Hadoop (HDFS, YARN, MapReduce)**
- **HBase**
- **Apache Airflow** (Scheduler, Webserver, Worker)
- **Python** (scripts de traitement des donnÃ©es)

### â–¶ï¸ Lancement du projet :

#### 1ï¸âƒ£ Cloner le dÃ©pÃ´t :
```bash
git clone <URL_REPO>
cd <nom_du_projet>
```

#### 1ï¸âƒ£ Lancer les conteneurs :
```
docker-compose up -d
```
#### 3ï¸âƒ£ AccÃ©der Ã  l'interface Airflow :
URL : http://localhost:8080

#### 4ï¸âƒ£ Visualiser HBase :
Utiliser hbase shell Ã  lâ€™intÃ©rieur du conteneur pour interroger les donnÃ©es traitÃ©es.

#### ğŸ›‘ ArrÃªter les conteneurs :
```
docker-compose down
```
